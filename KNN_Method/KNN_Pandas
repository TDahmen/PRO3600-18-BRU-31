#!/usr/bin/env python3
# -*- coding: utf-8 -*-

## /!\ Il est nécessaire de placer les deus bases de données "gamesData.npy" et "oneHotEncoded.npy" dans le même répertoire que ce script Python

import numpy as np  
import matplotlib.pyplot as plt  
import pandas as pd  

gamesData = np.load('gamesData.npy') ## Importation des 2 bases de données
oneHotEncoded = np.load('oneHotEncoded.npy')

dataset1 = pd.DataFrame(gamesData) ## Conversion en DataFrame utilisables par Panda
dataset2 = pd.DataFrame(oneHotEncoded)

X = dataset1 
y = dataset2 

from sklearn.model_selection import train_test_split  ## Création des tables d'entrainements et des tables de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000/389960)


from sklearn.preprocessing import StandardScaler  ## Normalistion des données permettant d'avoir de meilleurs résultats
scaler = StandardScaler()  
scaler.fit(X_train)

X_train = scaler.transform(X_train)  
X_test = scaler.transform(X_test) 


from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5) ## On fixe notre K de façon aléatoire
classifier.fit(X_train, y_train)  

y_pred = classifier.predict(X_test) ## On effectue la prédiction

from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test, y_pred))  ## On affiche les matrices de confusion permettant de mesurer l'optimalité du K
print(classification_report(y_test, y_pred)) ## On affiche le rapport final comparant la prédiction avec la base de donnée

## On va ensuite ajuster le K de façon à se rapprocher des 100%
